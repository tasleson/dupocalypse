# Introduction
The thin provisioning device mapper target has always had the facility to take metadata snapshots for use by userland processes.  The intention was this would be used for incremental backups and health monitoring.  I didn't write any tools to use this (other than thin_ls).  The number of third party users is very small; one product uses metadata snaps to do remote replication, and I just discovered [wyng-backup](https://github.com/tasket/wyng-backup), which has similar aims to dupocalypse.

Meanwhile, some users of thin provisioning are using the thin snapshot facility very frequently to take a form of rolling backup.  This results in thousands of snapshots being present in the pool, resulting in less space and more write amplification as we continually break sharing when performing writes.

dupocalypse provides a way to archive any block device, provided it is not in use.  It works particularly well with dm-thin, since thin snapshots can be used to effectively archive live devices.  Metadata snapshots are used to do incremental archiving, and to maximise sharing when restoring a volume to a pool.

Archived data is deduplicated and compressed then stored within a file system.

The tool is small, probably comprising ~5k lines of code when finished.  Design decisions have been taken to keep things simple.  Giving us less code to maintain, and reducing chances of archive corruption due to software error.  In particular the core archive files are either only appended to or only written once.  By default _pack_ operations are verified to guarantee the stream in the archive matches that of the original device.  The downside of these decisions is we do not support deleting streams from an archive.  This restriction can be ameliorated by migrating streams between archives, which also has the beneficial side effect of defragging streams.

This document describes at the high level the dupocalypse implementation.  Read it if you wish to understand how it works.

# Slab files
dupocalypse stores its data in _slab files_.  These files consist of a simple header followed by multiple _slabs_.  Each slab is an opaque chunk of data, of a size chosen by the client (typically  4M).   The client may only read an existing slab, or append a new one; there is no way to modify an existing slab.

Slabs are always checksummed, so corruption at the slab level is detected when the client reads the slab.

Slabs may optionally be compressed.  This is a file level option; either all the slabs in a file are compressed, or none.

The whole slab file may be encrypted.  This is to support users who wish to store their archives on cloud storage. 

# Dedup

There are two classes of data deduplication:

- Online; deduplicates live data.  This is a _very_ hard problem, having to cope with maintaining large indexes in kernel space, keeping latency of individual IOs low and support random access IO.  Typically fixed size blocks are used.
- Offline; which generally dedups a sequential stream of data.  This is a much simpler problem.  By maintaining stream locality for the data blocks and hashes it's much more likely that the data we need is already in memory.  The lack of random access IO also means we can use variable block sizes which dramatically improves the amount of duplication found.  Everything is in userland, meaning we don't have to allocate all in core data structures up front.

dupocalypse is an offline tool of course.  Two slab files are generated by dedup, one to store the data, and another which stores the hashes + data offsets.

Stream processing goes through these phases:

## Split
When the archive is created a block size is specified (currently defaulting to 4k).  This block size is just an average, the splitter code actually decides where to split based on a hash function that only depends on the previous _block size_ bytes.  This means boundaries for similar data will tend to align allowing us to find more duplicates.

## Hash
Each block has a hash calculated for it.  This needs to be a strong crypto hash since we assume that if two blocks have the same hash then they contain the same data (if this assumption ever fails then the verify stage of packing will detect it).  I'm currently using the Blake256 crypto hash.  This is popular due to it's fast performance.

## Lookup
We see if the hash for the current block has been seen before.  If it has, we store a reference to the earlier block in the stream.  If not, we add the data to the current data slab, and the hash to the current hash slab.


# Indexer
The indexer is a data structure that answers two related questions:

- a) Have we seen this hash before?
- b) If so where would I find the data?

It's worth noting that the indexer is allowed to be fallible, if it gives the wrong answer we may miss some duplicate data.

For performance reasons the indexer must stay in memory.  But since an archive will contain large numbers of unique blocks we must take care to minimise the memory used.

Question (a) is answered using a Cuckoo filter.  This uses 18 bits per block entry and offers a false positive rate of less than 1 in 10,000.  At the moment the Cuckoo Filter has to be sized to hold a power of 2 number of entries.  However I'm considering using the techniques from [this](https://www.vldb.org/pvldb/vol13/p197-wang.pdf) paper to allow finer grain resizing of the filter.

Question (b) is answered by augmenting the Cuckoo filter with a 32 bit slab index.  This index tells us which slab the hash would be contained in.  The dedup process then pages in this slab to the _hash cache_ if it's not already present, and checks the full 256bit hash.

When choosing a block size for an archive it's important to consider how much memory the system has to hold indexer data, and how much unique data you're expecting to put in the archive.  Here's some example numbers assuming we have 1T of unique data to store in the archive:

| block size | Index memory size |
| ---             | ---                           |
|1k | 6.4G|
|4k | 1.6G|
|16k | 400M|

In addition to the indexer memory the pack process will need a lot of space for the _hash cache_ (see below).

# Hash cache
When archiving a stream an in core BTree is maintained mapping 256bit hashes to data locations.  This tree doesn't contain all hashes seen, just those for recently visited slabs.  The amount of memory devoted to the hash cache is configurable.  This defaults to 1G.

# Compression
dupocalypse uses zlib with default settings.

# Streams
To reconstruct a stream we need to know which data blocks it is composed of.  Simply writing a list of data slab index + offset to the stream file would consume a lot of space.  So we use various tricks to compress it.

- Often a run of adjacent data blocks will be emitted at once (stream locality).  Run length encoding is an obvious saving.
- Unprovisioned regions of a thin volume need their own representation.
- Surprisingly a lot of blocks contain bytes with identical values.  So we special case these.

Snapshots complicate things.  We know they will share a lot of data entries with the snapshot origin (assuming it's already archived).  One way to take advantage of this is to store references in the stream to an earlier stream, effectively saying something like "the next 63M are identical to stream X and this point".  The downside to this scheme is we start having to read multiple stream files concurrently; potentially very many if people are using a rolling snapshot scenario.  So then we start having to start developing heuristics to say when to use back references, and when to write out the stream in full.  This is too complicated, so instead dupocalypse just dedups the stream data itself, and lets the dedup engine find the commonality.

The stream is represented as a sequence of instructions for a tiny abstract machine.  The machine has 16 registers arranged in a stack.  Each register consists of a data slab, and offset into the slab.  Some instructions manipulate the stack.  Others adjust the slab or offset fields of the top of the stack.  Finally emit instructions causes runs of data to be emitted and automatically increment the offset field of the top cursor.

A stack is used, rather than a named register file, in order to increase the commonality between related streams and hence compress more when they're deduped.

The _dump-stream_ command can be used to inspect a stream.  It lists in a table the address, instruction, and the state of the stack assuming the stream has been executed up to this point.  here's the start of a 14 level deep snapshot (I've truncated the stack to improve formatting):

```
0000000000        dup 15         0:0 0:0 0:0 0:0     
0000000001      s.add 1538       1538:0 0:0 0:0 0:0  
0000000002       emit 16         1538:16 0:0 0:0 0:0 
0000000003        dup 15         1538:16 1538:16 0:0 0:0 
0000000004      s.add -349       1189:16 1538:16 0:0 0:0 
0000000005      o.add 1          1189:17 1538:16 0:0 0:0 
0000000006       emit 1          1189:18 1538:16 0:0 0:0 
0000000007        rot 14         1538:16 1189:18 0:0 0:0 
0000000008      s.add -113       1425:16 1189:18 0:0 0:0 
0000000009      o.add -1         1425:15 1189:18 0:0 0:0 
000000000a       emit 2          1425:17 1189:18 0:0 0:0 
000000000b        rot 14         1189:18 1425:17 0:0 0:0 
000000000c      o.add 2          1189:20 1425:17 0:0 0:0 
000000000d       emit 1          1189:21 1425:17 0:0 0:0 
000000000e        dup 15         1189:21 1189:21 1425:17 0:0 
000000000f      s.add -120       1069:21 1189:21 1425:17 0:0 
0000000010      o.add -2         1069:19 1189:21 1425:17 0:0 
0000000011       emit 1          1069:20 1189:21 1425:17 0:0 
0000000012        rot 13         1425:17 1069:20 1189:21 0:0 
0000000013      s.add 113        1538:17 1069:20 1189:21 0:0 
0000000014      o.add -1         1538:16 1069:20 1189:21 0:0 
0000000015       emit 2          1538:18 1069:20 1189:21 0:0 
0000000016        rot 13         1189:21 1538:18 1069:20 0:0 
0000000017      s.add 117        1306:21 1538:18 1069:20 0:0 
0000000018      o.add 1          1306:22 1538:18 1069:20 0:0 
0000000019       emit 1          1306:23 1538:18 1069:20 0:0 
000000001a        rot 14         1538:18 1306:23 1069:20 0:0 
000000001b       emit 2          1538:20 1306:23 1069:20 0:0 
000000001c        dup 15         1538:20 1538:20 1306:23 1069:20 
000000001d      s.add -113       1425:20 1538:20 1306:23 1069:20 
000000001e      o.add -2         1425:18 1538:20 1306:23 1069:20 
000000001f       emit 5          1425:23 1538:20 1306:23 1069:20 
0000000020        rot 13         1306:23 1425:23 1538:20 1069:20 
0000000021      s.add -117       1189:23 1425:23 1538:20 1069:20 
0000000022      o.add 8          1189:31 1425:23 1538:20 1069:20 
0000000023       emit 2          1189:33 1425:23 1538:20 1069:20 
0000000024        rot 12         1069:20 1189:33 1425:23 1538:20 
0000000025      o.add 14         1069:34 1189:33 1425:23 1538:20 
0000000026       emit 2          1069:36 1189:33 1425:23 1538:20 
0000000027        rot 14         1189:33 1069:36 1425:23 1538:20 
0000000028      s.add 117        1306:33 1069:36 1425:23 1538:20 
0000000029      o.add -2         1306:31 1069:36 1425:23 1538:20 
000000002a       emit 1          1306:32 1069:36 1425:23 1538:20 
000000002b        rot 12         1538:20 1306:32 1069:36 1425:23 
000000002c       emit 60         1538:80 1306:32 1069:36 1425:23 
000000002d        rot 13         1069:36 1538:80 1306:32 1425:23 
000000002e      o.add 58         1069:94 1538:80 1306:32 1425:23 
000000002f       emit 1          1069:95 1538:80 1306:32 1425:23 
0000000030        rot 14         1538:80 1069:95 1306:32 1425:23 
```

Finally _dump-stream_ gives some stats on how often each instruction type was used:

```
Instruction frequencies:

                rot 73182     
              emit4 71810     
     offset_delta12 29184     
      offset_delta4 17316     
             emit12 2481      
        slab_delta4 1779      
       slab_delta12 562       
                dup 219       
             fill16 144       
              pos32 2         
           set-fill 0         
              fill8 0         
             fill32 0         
             fill64 0         
          unmapped8 0         
         unmapped16 0         
         unmapped32 0         
         unmapped64 0         
             slab16 0         
             slab32 0         
            offset4 0         
           offset12 0         
           offset20 0         
             emit20 0         
              pos64 0
```

The more duplicate data we find for a stream, the more complicated the stream program will be, and so the more space it will take up.  For instance here are the stream file sizes (before dedup) of archiving tar files containing kernel source for v5.0 -> v5.13 (each file is just under 1G):

```
-rw-r--r-- 1 ejt ejt   5284 Apr 12 10:05 test-archive/streams/affa251b5adf3720/stream
-rw-r--r-- 1 ejt ejt  45388 Apr 12 10:05 test-archive/streams/d4dc32b7ce5ba564/stream
-rw-r--r-- 1 ejt ejt  58370 Apr 12 10:05 test-archive/streams/9887832106101aeb/stream
-rw-r--r-- 1 ejt ejt  72119 Apr 12 10:06 test-archive/streams/9fab1a8d530e37fa/stream
-rw-r--r-- 1 ejt ejt  81200 Apr 12 10:06 test-archive/streams/c18311a4030502e3/stream
-rw-r--r-- 1 ejt ejt  88732 Apr 12 10:06 test-archive/streams/1495a6c7f8172ec4/stream
-rw-r--r-- 1 ejt ejt  95597 Apr 12 10:07 test-archive/streams/377eca0ba5217726/stream
-rw-r--r-- 1 ejt ejt 100770 Apr 12 10:07 test-archive/streams/a98de0f3f954dc13/stream
-rw-r--r-- 1 ejt ejt 103729 Apr 12 10:07 test-archive/streams/ea923330cafaae7d/stream
-rw-r--r-- 1 ejt ejt 109686 Apr 12 10:08 test-archive/streams/0fcdd6a4585a8afb/stream
-rw-r--r-- 1 ejt ejt 115406 Apr 12 10:08 test-archive/streams/41caf442bb453ff4/stream
-rw-r--r-- 1 ejt ejt 123890 Apr 12 10:09 test-archive/streams/f8ca25fe34a15804/stream
-rw-r--r-- 1 ejt ejt 129307 Apr 12 10:09 test-archive/streams/994733244c4b86f3/stream
-rw-r--r-- 1 ejt ejt 132505 Apr 12 10:10 test-archive/streams/5be10fc74c1a2d27/stream

```

As more duplicates from earlier streams are found the streams get more and more fragmented.  This is just the nature of dedup.

# Packing process
The packer reads a stream from either a file, thick device or thin device and sends it through the above dedup + compress process.

Thin devices only have provisioned regions packed.  If packing a snapshot delta then only those regions that have different mappings will be packed, otherwise it'll be assumed to be identical to the previously archived device.

A single thread handles reading, splitting, hashing and deduping.  Multiple back end threads (4?) compress data, and then a single writer thread writes to the slab files.  Assuming the machine has sufficient IO bandwidth the above process is expected to archive at a rate of ~400M per second (we're not there yet, but the splitter is badly written atm).  If a large stream is being processed and there are spare cores, then I want to create multiple instances of the above threads and archive different regions of the stream in parallel.  So in theory we should be able to saturate the bandwidth of modern NVMe devices.


# Unpacking process

Unpacking consists of executing the stream instructions which will write the relevant data entries.

Snapshots often jump back and forth between the same slabs as they write data from the origin, then regions that changed in the snapshot.  To speed this up an LRU cache of visited data slabs is maintained.  This cache has a configurable size, defaulting to 1G.

I suspect it would be worth pre-reading the stream to calculate an IO schedule for the data slabs.  This would let us read up coming slabs in the background and drop slabs that we knew were no longer needed.

We need to have multiple decompressor threads to stop zlib being a bottle neck.

When restoring to a thin volume the mappings of the destination are read.  If the stream writes to an already provisioned region then the data already on the disk is read and compared with what we are intending to write.  If it's identical the write is elided.  This lets us preserve data sharing within the thin pool as much as possible.  Note the destination does not have to be a direct ancestor of the stream, any related thin device will work such as the current head of a rolling snapshot scenario.
